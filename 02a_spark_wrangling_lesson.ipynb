{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "https://docs.google.com/presentation/d/1v54Tr4POZj9K4zsaHOn7U22rzdSlTXsyCgE5lfhd-rA/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema functions import\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use San Antonio's 311 call data for this lesson and exercises. Dowload the CSV data from Google classroom\n",
    "\n",
    "https://classroom.google.com/u/0/w/Mzg3MTg5NzU1Njk1/tc/Mzg3MTg5NzU1NzE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV file to spark df\n",
    "source = (spark.read.csv(\"csv_files/source.csv\",\n",
    "                     sep=\",\",\n",
    "                     header=True,\n",
    "                     inferSchema=True)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|source_id|     source_username|\n",
      "+---------+--------------------+\n",
      "|   100137|    Merlene Blodgett|\n",
      "|   103582|         Carmen Cura|\n",
      "|   106463|     Richard Sanchez|\n",
      "|   119403|      Betty De Hoyos|\n",
      "|   119555|      Socorro Quiara|\n",
      "|   119868| Michelle San Miguel|\n",
      "|   120752|      Eva T. Kleiber|\n",
      "|   124405|           Lori Lara|\n",
      "|   132408|       Leonard Silva|\n",
      "|   135723|        Amy Cardenas|\n",
      "|   136202|    Michelle Urrutia|\n",
      "|   136979|      Leticia Garcia|\n",
      "|   137943|    Pamela K. Baccus|\n",
      "|   138605|        Marisa Ozuna|\n",
      "|   138650|      Kimberly Green|\n",
      "|   138650|Kimberly Green-Woods|\n",
      "|   138793| Guadalupe Rodriguez|\n",
      "|   138810|       Tawona Martin|\n",
      "|   139342|     Jessica Mendoza|\n",
      "|   139344|        Isis Mendoza|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at the spark df\n",
    "source.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|source_id|     source_username|\n",
      "+---------+--------------------+\n",
      "|   100137|    Merlene Blodgett|\n",
      "|   103582|         Carmen Cura|\n",
      "|   106463|     Richard Sanchez|\n",
      "|   119403|      Betty De Hoyos|\n",
      "|   119555|      Socorro Quiara|\n",
      "|   119868| Michelle San Miguel|\n",
      "|   120752|      Eva T. Kleiber|\n",
      "|   124405|           Lori Lara|\n",
      "|   132408|       Leonard Silva|\n",
      "|   135723|        Amy Cardenas|\n",
      "|   136202|    Michelle Urrutia|\n",
      "|   136979|      Leticia Garcia|\n",
      "|   137943|    Pamela K. Baccus|\n",
      "|   138605|        Marisa Ozuna|\n",
      "|   138650|      Kimberly Green|\n",
      "|   138650|Kimberly Green-Woods|\n",
      "|   138793| Guadalupe Rodriguez|\n",
      "|   138810|       Tawona Martin|\n",
      "|   139342|     Jessica Mendoza|\n",
      "|   139344|        Isis Mendoza|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to read in data:\n",
    "\n",
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"csv_files/source.csv\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Schemas\n",
    "Spark includes a concept of a data schema  \n",
    "Specify the types of our data ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(source_id,StringType,true),StructField(source_username,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting data schema object\n",
    "StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Read csv, but now we specify >> the schema:\n",
    "source = spark.read.csv(\"csv_files/source.csv\", header = True, schema = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- source_username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the schema\n",
    "source.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to a destination using .write property\n",
    "source.write.json(\"source_json\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the case.csv file\n",
    "\n",
    "df = spark.read.csv(\"csv_files/case.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 14 columns and 841704 rows.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "'There are ' + str(len(df.columns)) + ' columns and ' + str(df.count()) + ' rows.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO| -2.012604167|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO| -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "|1014127335|     1/1/18 1:29|     1/2/18 8:13|1/17/18 8:30|       NO| -15.01148148|        YES|Code Enforcement|Front Or Side Yar...|16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|               3|\n",
      "|1014127336|     1/1/18 1:34|    1/1/18 13:29| 1/1/18 4:34|      YES|  0.372164352|        YES|Field Operations|Animal Cruelty(Cr...|      0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|               7|\n",
      "|1014127337|     1/1/18 6:28|    1/1/18 14:38|1/31/18 8:30|       NO| -29.74398148|        YES|         Signals|Traffic Signal Op...|30.08446759|     Closed| svcCRMSS|BANDERA RD and BR...|               7|\n",
      "|1014127338|     1/1/18 6:57|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70673611|        YES|Code Enforcement|Front Or Side Yar...|16.06429398|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127339|     1/1/18 6:58|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70662037|        YES|Code Enforcement|Front Or Side Yar...| 16.0637963|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127340|     1/1/18 6:58|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70662037|        YES|Code Enforcement|Right Of Way/Side...|16.06333333|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127341|     1/1/18 6:59|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70649306|        YES|Code Enforcement|Front Or Side Yar...| 16.0628588|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127342|     1/1/18 7:00|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70649306|        YES|Code Enforcement|Front Or Side Yar...|16.06237269|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127343|     1/1/18 7:02|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70636574|        YES|Code Enforcement|Right Of Way/Side...|16.06104167|     Closed| svcCRMSS|10133  FIGARO CAN...|               4|\n",
      "|1014127344|     1/1/18 7:02|    1/2/18 15:33|1/17/18 8:30|       NO|    -14.70625|        YES|Code Enforcement|Front Or Side Yar...|16.06059028|     Closed| svcCRMSS|10129  BOXING PAS...|               4|\n",
      "|1014127345|     1/1/18 7:03|    1/2/18 15:32|1/17/18 8:30|       NO| -14.70636574|        YES|Code Enforcement|Front Or Side Yar...|16.06011574|     Closed| svcCRMSS|10129  BOXING PAS...|               4|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 14 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at first three records\n",
    "df.show(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('SLA_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#datatypes?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to do:**\n",
    "\n",
    "1. **Rename Columns:**\n",
    "    - 'SLA_due_date -> case_due_date\n",
    "\n",
    "\n",
    "\n",
    "2. **Correct Data Types:**\n",
    "    - case_closed and case_late to boolean\n",
    "    - council_district as a string\n",
    "    - case_opened_date, case_closed_date and case_due_date to datetime format\n",
    "\n",
    "\n",
    "3. **Data Transformation:**\n",
    "    - request_address: trim and lowercase\n",
    "    - format council district with leading zeros\n",
    "    - convert the number of days a case is late to a number of weeks\n",
    "    \n",
    "    \n",
    "4. **New features:**\n",
    "    - zip_code : extract from address\n",
    "    - case_age \n",
    "    - days_to_closed\n",
    "    - case_lifetime\n",
    "    \n",
    "    \n",
    "5. **Join cases data with department data:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename 'SLA_due_date' to 'case_due_date' using .withColumnRenamed\n",
    "df.withColumnRenamed('SLA_due_date', 'case_due_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing over original df\n",
    "df = df.withColumnRenamed('SLA_due_date', 'case_due_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|case_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|       NO| -998.5087616|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|       NO| -2.012604167|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df preview\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Data Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('SLA_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|      YES|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correct data types: case_closed and case_late to boolean\n",
    "\n",
    "# preview\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: boolean, num_days_late: double, case_closed: boolean, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use .withColumn to change columns from string to boolean values\n",
    "df.withColumn('case_closed', expr('case_closed == \"YES\"'))\\\n",
    ".withColumn('case_late', expr('case_late == \"YES\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|       NO|\n",
      "|        YES|      YES|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the columns\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|council_district|\n",
      "+----------------+\n",
      "|               5|\n",
      "|               3|\n",
      "|               3|\n",
      "|               3|\n",
      "+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# council_district cast as string\n",
    "df.select('council_district').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# council_district as a string instead of int\n",
    "df = df.withColumn('council_district', col('council_district').cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('case_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'string')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "|     1/1/18 1:34|    1/1/18 13:29|  1/1/18 4:34|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert case_opened_date, case_closed_date and case_due_date to datetime format\n",
    "\n",
    "df.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string, case_opened_data: timestamp, case_closed_data: timestamp, case_due_data: timestamp]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_timestamp, fmt\n",
    "\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "\n",
    "df.withColumn('case_opened_data', to_timestamp('case_opened_date', fmt))\\\n",
    "df.withColumn('case_closed_data', to_timestamp('case_closed_date', fmt))\\\n",
    "df.withColumn('case_due_data', to_timestamp('case_due_date', fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning to df\n",
    "df = df.withColumn('case_opened_data', to_timestamp('case_opened_date', fmt))\\\n",
    ".withColumn('case_closed_data', to_timestamp('case_closed_date', fmt))\\\n",
    ".withColumn('case_due_data', to_timestamp('case_due_date', fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "|     1/1/18 1:34|    1/1/18 13:29|  1/1/18 4:34|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the three columns again\n",
    "df.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('case_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'string'),\n",
       " ('case_opened_data', 'timestamp'),\n",
       " ('case_closed_data', 'timestamp'),\n",
       " ('case_due_data', 'timestamp')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  EL PASO ST,...|\n",
      "|2215  GOLIAD RD, ...|\n",
      "|102  PALFREY ST W...|\n",
      "|114  LA GARDE ST,...|\n",
      "|734  CLEARVIEW DR...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('request_address').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|request_address                      |\n",
      "+-------------------------------------+\n",
      "|2315  EL PASO ST, San Antonio, 78207 |\n",
      "|2215  GOLIAD RD, San Antonio, 78223  |\n",
      "|102  PALFREY ST W, San Antonio, 78223|\n",
      "|114  LA GARDE ST, San Antonio, 78223 |\n",
      "|734  CLEARVIEW DR, San Antonio, 78228|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# request_address: trim and lowercase\n",
    "#     False removes truncations\n",
    "df.select('request_address').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string, case_opened_data: timestamp, case_closed_data: timestamp, case_due_data: timestamp]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request_address: trim and lowercase\n",
    "\n",
    "df.withColumn('request_address', lower(trim('request_address')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('request_address', lower(trim('request_address')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|request_address                      |\n",
      "+-------------------------------------+\n",
      "|2315  el paso st, san antonio, 78207 |\n",
      "|2215  goliad rd, san antonio, 78223  |\n",
      "|102  palfrey st w, san antonio, 78223|\n",
      "|114  la garde st, san antonio, 78223 |\n",
      "|734  clearview dr, san antonio, 78228|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('request_address').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|num_days_late|\n",
      "+-------------+\n",
      "| -998.5087616|\n",
      "| -2.012604167|\n",
      "| -3.022337963|\n",
      "| -15.01148148|\n",
      "|  0.372164352|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the number of days a case is late to a number of weeks\n",
    "\n",
    "# viewing current column for number of days late\n",
    "df.select('num_days_late').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('case_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'string'),\n",
       " ('case_opened_data', 'timestamp'),\n",
       " ('case_closed_data', 'timestamp'),\n",
       " ('case_due_data', 'timestamp')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      num_weeks_late|\n",
      "+--------------------+\n",
      "|        -142.6441088|\n",
      "|        -0.287514881|\n",
      "|-0.43176256614285713|\n",
      "| -2.1444973542857144|\n",
      "|0.053166335999999995|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code to get new col\n",
    "df.withColumn('num_weeks_late', expr('num_days_late / 7')).select('num_weeks_late').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+-------------------+-------------------+-------------------+--------------+\n",
      "|   case_id|case_opened_date|case_closed_date|case_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|SLA_days|case_status|source_id|     request_address|council_district|   case_opened_data|   case_closed_data|      case_due_data|num_weeks_late|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+-------------------+-------------------+-------------------+--------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|       NO| -998.5087616|        YES|Field Operations|        Stray Animal|   999.0|     Closed| svcCRMLS|2315  el paso st,...|               5|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|  -142.6441088|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+-------------------+-------------------+-------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# writing over df, adding new weeks col to original data\n",
    "df = df.withColumn('num_weeks_late', expr('num_days_late / 7'))\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string, case_opened_data: timestamp, case_closed_data: timestamp, case_due_data: timestamp, num_weeks_late: double]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use format_string function to pad zeros for council_district\n",
    "df.withColumn('council_district', format_string('%03d', col('council_district').cast('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('case_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int'),\n",
       " ('case_opened_data', 'timestamp'),\n",
       " ('case_closed_data', 'timestamp'),\n",
       " ('case_due_data', 'timestamp'),\n",
       " ('num_weeks_late', 'double')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn('council_district', format_string('%03d', col('council_district')).cast('int'))\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, case_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int, case_opened_data: timestamp, case_closed_data: timestamp, case_due_data: timestamp, num_weeks_late: double, zipcode: string]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column for zipcode:\n",
    "\n",
    "df.withColumn('zipcode', expr('right(request_address, 5)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('zipcode', expr('right(request_address, 5)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------+\n",
      "|request_address                      |zipcode|\n",
      "+-------------------------------------+-------+\n",
      "|2315  el paso st, san antonio, 78207 |78207  |\n",
      "|2215  goliad rd, san antonio, 78223  |78223  |\n",
      "|102  palfrey st w, san antonio, 78223|78223  |\n",
      "|114  la garde st, san antonio, 78223 |78223  |\n",
      "+-------------------------------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('request_address', 'zipcode').show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # with regex\n",
    "# regex = r'(/d+$)\n",
    "\n",
    "# df.withColumn('zipcode', regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case_age: How old the case is; the difference in days between when the case was opened and the current day  \n",
    "days_to_closed: The number of days between when the case was opened and when it was closed  \n",
    "case_lifetime: Number of days between when the case was opened and when it was closed, if the case is still open, the number of days since the case was opened  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(NOT case_closed)' due to data type mismatch: argument 1 requires boolean type, however, 'case_closed' is of string type.; line 1 pos 0;\n'Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, case_age#1880, days_to_closed#1901, CASE WHEN NOT case_closed#377 THEN case_age#1880 ELSE days_to_closed#1901 END AS case_lifetime#1923]\n+- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, case_age#1880, datediff(cast(case_closed_date#373 as date), cast(case_opened_date#372 as date)) AS days_to_closed#1901]\n   +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, datediff(cast(current_timestamp() as date), cast(case_opened_date#372 as date)) AS case_age#1880]\n      +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, right(request_address#1142, 5) AS zipcode#1732]\n         +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1674) as int) AS council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n            +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1648) as int) AS council_district#1674, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n               +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1628) as int) AS council_district#1648, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                  +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1602) as int) AS council_district#1628, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                     +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1576) as int) AS council_district#1602, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                        +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#785) as int) AS council_district#1576, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                           +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, (num_days_late#376 / cast(7 as double)) AS num_weeks_late#1432]\n                              +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, (num_days_late#376 / cast(7 as double)) AS num_weeks_late#1326]\n                                 +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, lower(trim(request_address#383, None)) AS request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073]\n                                    +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#1037, case_closed_data#1055, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_data#1073]\n                                       +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#1037, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_data#1055, case_due_data#997]\n                                          +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_data#1037, case_closed_data#980, case_due_data#997]\n                                             +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#964, case_closed_data#980, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_data#997]\n                                                +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#964, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_data#980]\n                                                   +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_data#964]\n                                                      +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, cast(council_district#384 as string) AS council_district#785]\n                                                         +- Project [case_id#371, case_opened_date#372, case_closed_date#373, SLA_due_date#374 AS case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#384]\n                                                            +- Relation [case_id#371,case_opened_date#372,case_closed_date#373,SLA_due_date#374,case_late#375,num_days_late#376,case_closed#377,dept_division#378,service_request_type#379,SLA_days#380,case_status#381,source_id#382,request_address#383,council_district#384] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-0f0d6d087302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m df = (\n\u001b[0;32m----> 4\u001b[0;31m     df.withColumn(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"case_age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"case_opened_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(NOT case_closed)' due to data type mismatch: argument 1 requires boolean type, however, 'case_closed' is of string type.; line 1 pos 0;\n'Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, case_age#1880, days_to_closed#1901, CASE WHEN NOT case_closed#377 THEN case_age#1880 ELSE days_to_closed#1901 END AS case_lifetime#1923]\n+- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, case_age#1880, datediff(cast(case_closed_date#373 as date), cast(case_opened_date#372 as date)) AS days_to_closed#1901]\n   +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, zipcode#1732, datediff(cast(current_timestamp() as date), cast(case_opened_date#372 as date)) AS case_age#1880]\n      +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432, right(request_address#1142, 5) AS zipcode#1732]\n         +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1674) as int) AS council_district#1693, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n            +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1648) as int) AS council_district#1674, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n               +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1628) as int) AS council_district#1648, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                  +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1602) as int) AS council_district#1628, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                     +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#1576) as int) AS council_district#1602, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                        +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, cast(format_string(%03d, council_district#785) as int) AS council_district#1576, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, num_weeks_late#1432]\n                           +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, (num_days_late#376 / cast(7 as double)) AS num_weeks_late#1432]\n                              +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073, (num_days_late#376 / cast(7 as double)) AS num_weeks_late#1326]\n                                 +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, lower(trim(request_address#383, None)) AS request_address#1142, council_district#785, case_opened_data#1037, case_closed_data#1055, case_due_data#1073]\n                                    +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#1037, case_closed_data#1055, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_data#1073]\n                                       +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#1037, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_data#1055, case_due_data#997]\n                                          +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_data#1037, case_closed_data#980, case_due_data#997]\n                                             +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#964, case_closed_data#980, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_data#997]\n                                                +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, case_opened_data#964, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_data#980]\n                                                   +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#785, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_data#964]\n                                                      +- Project [case_id#371, case_opened_date#372, case_closed_date#373, case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, cast(council_district#384 as string) AS council_district#785]\n                                                         +- Project [case_id#371, case_opened_date#372, case_closed_date#373, SLA_due_date#374 AS case_due_date#599, case_late#375, num_days_late#376, case_closed#377, dept_division#378, service_request_type#379, SLA_days#380, case_status#381, source_id#382, request_address#383, council_district#384]\n                                                            +- Relation [case_id#371,case_opened_date#372,case_closed_date#373,SLA_due_date#374,case_late#375,num_days_late#376,case_closed#377,dept_division#378,service_request_type#379,SLA_days#380,case_status#381,source_id#382,request_address#383,council_district#384] csv\n"
     ]
    }
   ],
   "source": [
    "#create three new columns 'case_age', 'days_to_closed', 'case_lifetime'\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " dept_division          | 311 Call Center               \n",
      " dept_name              | Customer Service              \n",
      " standardized_dept_name | Customer Service              \n",
      " dept_subject_to_SLA    | YES                           \n",
      "-RECORD 1-----------------------------------------------\n",
      " dept_division          | Brush                         \n",
      " dept_name              | Solid Waste Management        \n",
      " standardized_dept_name | Solid Waste                   \n",
      " dept_subject_to_SLA    | YES                           \n",
      "-RECORD 2-----------------------------------------------\n",
      " dept_division          | Clean and Green               \n",
      " dept_name              | Parks and Recreation          \n",
      " standardized_dept_name | Parks & Recreation            \n",
      " dept_subject_to_SLA    | YES                           \n",
      "-RECORD 3-----------------------------------------------\n",
      " dept_division          | Clean and Green Natural Areas \n",
      " dept_name              | Parks and Recreation          \n",
      " standardized_dept_name | Parks & Recreation            \n",
      " dept_subject_to_SLA    | YES                           \n",
      "-RECORD 4-----------------------------------------------\n",
      " dept_division          | Code Enforcement              \n",
      " dept_name              | Code Enforcement Services     \n",
      " standardized_dept_name | DSD/Code Enforcement          \n",
      " dept_subject_to_SLA    | YES                           \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the dept.csv file:\n",
    "\n",
    "dept = spark.read.csv(\"csv_files/dept.csv\", header = True, inferSchema = True)\n",
    "dept.show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept.show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       dept_division|\n",
      "+--------------------+\n",
      "|       Miscellaneous|\n",
      "|         Solid Waste|\n",
      "|    Field Operations|\n",
      "|             Streets|\n",
      "|    Waste Collection|\n",
      "|          District 7|\n",
      "|Code Enforcement ...|\n",
      "|         District 10|\n",
      "|              Vector|\n",
      "|        Reservations|\n",
      "|   Dangerous Premise|\n",
      "|     311 Call Center|\n",
      "|               Brush|\n",
      "|Dangerous Premise...|\n",
      "|Code Enforcement ...|\n",
      "|Traffic Engineeri...|\n",
      "|          District 2|\n",
      "|             Signals|\n",
      "|Engineering Division|\n",
      "|Director's Office...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|           dept_name|\n",
      "+--------------------+\n",
      "|    Customer Service|\n",
      "|Solid Waste Manag...|\n",
      "|Parks and Recreation|\n",
      "|Parks and Recreation|\n",
      "|Code Enforcement ...|\n",
      "|Code Enforcement ...|\n",
      "|                null|\n",
      "|Code Enforcement ...|\n",
      "|Code Enforcement ...|\n",
      "|Trans & Cap Impro...|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|        City Council|\n",
      "|Development Services|\n",
      "|        Metro Health|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept.select('dept_division').distinct().show()\n",
    "dept.select('dept_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'dept_subject_to_SLA' is ambiguous, could be: dept_subject_to_SLA, dept_subject_to_SLA.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-f26dd1f54594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m df = (\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# left join on dept_division\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dept_division\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Reference 'dept_subject_to_SLA' is ambiguous, could be: dept_subject_to_SLA, dept_subject_to_SLA."
     ]
    }
   ],
   "source": [
    "# join the df and dept dataframe using 'dept_division' as common key\n",
    "# drop columns as needed (keep standardized_dept_name)\n",
    "# convert dept_subject_to_SLA to boolean\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    # left join on dept_division\n",
    "    .join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o600.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 132) (10.10.2.146 executor driver): java.util.IllegalFormatConversionException: d != org.apache.spark.unsafe.types.UTF8String\n\tat java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4426)\n\tat java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2938)\n\tat java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2892)\n\tat java.base/java.util.Formatter.format(Formatter.java:2673)\n\tat java.base/java.util.Formatter.format(Formatter.java:2609)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:338)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:366)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:338)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.util.IllegalFormatConversionException: d != org.apache.spark.unsafe.types.UTF8String\n\tat java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4426)\n\tat java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2938)\n\tat java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2892)\n\tat java.base/java.util.Formatter.format(Formatter.java:2673)\n\tat java.base/java.util.Formatter.format(Formatter.java:2609)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d79f1d73e9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o600.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 132) (10.10.2.146 executor driver): java.util.IllegalFormatConversionException: d != org.apache.spark.unsafe.types.UTF8String\n\tat java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4426)\n\tat java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2938)\n\tat java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2892)\n\tat java.base/java.util.Formatter.format(Formatter.java:2673)\n\tat java.base/java.util.Formatter.format(Formatter.java:2609)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:338)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:366)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:338)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.util.IllegalFormatConversionException: d != org.apache.spark.unsafe.types.UTF8String\n\tat java.base/java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:4426)\n\tat java.base/java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2938)\n\tat java.base/java.util.Formatter$FormatSpecifier.print(Formatter.java:2892)\n\tat java.base/java.util.Formatter.format(Formatter.java:2673)\n\tat java.base/java.util.Formatter.format(Formatter.java:2609)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
